{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from ./data/ to S3 exploration notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import boto3\n",
    "import zipfile\n",
    "import shutil\n",
    "import progressbar\n",
    "\n",
    "# Notebook specific\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Exploring data\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connections\n",
    "region = 'us-east-1'\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of existing buckets\n",
    "response = s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Existing buckets:\n  arxiv-etl\n  jbma-data-engineering-us-east\n"
    }
   ],
   "source": [
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Bucket exists: True\n"
    }
   ],
   "source": [
    "# Check if bucket already exists\n",
    "bucket_name = 'arxiv-etl'\n",
    "bucket_exists = False\n",
    "for obj in response['Buckets']:\n",
    "    if obj['Name'] == bucket_name:\n",
    "        bucket_exists = True\n",
    "print(f\"Bucket exists: {bucket_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket if it doesn't exist\n",
    "if not bucket_exists:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} created\")\n",
    "else: \n",
    "    print(f\"Bucket {bucket_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and upload data to bucket - replace existing files as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip files to new folder\n",
    "# https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
    "path_to_zip_file = '../data/arxiv.zip'\n",
    "directory_to_extract_to = '../data/loading/'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Filename:  arxiv-metadata-oai-snapshot.json \n\n{'id': '0704.0001', 'submitter': 'Pavel Nadolsky', 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\", 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies', 'comments': '37 pages, 15 figures; published version', 'journal-ref': 'Phys.Rev.D76:013009,2007', 'doi': '10.1103/PhysRevD.76.013009', 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n', 'report-no': 'ANL-HEP-PR-07-12', 'categories': ['hep-ph'], 'versions': ['v1', 'v2']}\n{'id': '0704.0002', 'submitter': 'Louis Theran', 'authors': 'Ileana Streinu and Louis Theran', 'title': 'Sparsity-certifying Graph Decompositions', 'comments': 'To appear in Graphs and Combinatorics', 'journal-ref': None, 'doi': None, 'abstract': '  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n', 'report-no': None, 'categories': ['math.CO cs.CG'], 'versions': ['v1', 'v2']}\n\n\n"
    }
   ],
   "source": [
    "# Lets look at those files using ijson to avoid loading entire files\n",
    "folder_name = '../data/loading/'\n",
    "directory = os.fsencode(folder_name)\n",
    "max_lines_to_load = 2\n",
    "\n",
    "# https://www.aylakhan.tech/?p=27\n",
    "\n",
    "def load_data(full_name, lines_to_load = 2):\n",
    "    with open(full_name, 'r') as f:\n",
    "        data  = []\n",
    "        lines = 0\n",
    "        for line in f: \n",
    "            data.append(json.loads(line))\n",
    "            lines += 1\n",
    "            if lines >= lines_to_load:\n",
    "                return data\n",
    "            \n",
    "for file in os.listdir(directory):\n",
    "    file_name = os.fsdecode(file)\n",
    "    full_name = os.path.join(folder_name, file_name)\n",
    "    file_size = Path(full_name).stat().st_size / 1024 / 1024 # Mb\n",
    "    if file_name.endswith(\".json\") and file_size > 500: # Only open large files \n",
    "        print(\"Filename: \", file_name, \"\\n\")\n",
    "        data = load_data(full_name, max_lines_to_load)\n",
    "        for line in data:\n",
    "            print(line)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Filename:  arxiv-metadata-oai-snapshot.json\nSample created for {file_name}\n"
    }
   ],
   "source": [
    "# Create sample for large file to work on subset initially for testing\n",
    "for file in os.listdir(directory):\n",
    "    file_name = os.fsdecode(file)\n",
    "    full_name = os.path.join(folder_name, file_name)\n",
    "    file_size = Path(full_name).stat().st_size / 1024 / 1024 # Mb\n",
    "    if file_name.endswith(\".json\") and file_size > 500: # Only open large files \n",
    "        print(\"Filename: \", file_name)\n",
    "        data = load_data(full_name, 150000) # Roughly 10 % of original\n",
    "        with open(full_name, \"w\") as outfile:\n",
    "            for item in data:\n",
    "                outfile.write(f\"{str(item)}\\n\")\n",
    "        print('Sample created for {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'../data/loading/subject-classifications.csv'"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Copy the classification data to loading folder to group it, which lets us sync the folder, and we can delete it after to save space\n",
    "classification_src = '../data/subject-classifications.csv'\n",
    "classification_dst = '../data/loading/subject-classifications.csv'\n",
    "shutil.copyfile(classification_src, classification_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(s3_client, folder_name, file_name, bucket_name):\n",
    "    # https://stackoverflow.com/questions/41827963/track-download-progress-of-s3-file-using-boto3-and-callbacks\n",
    "    full_name = os.path.join(folder_name, file_name)\n",
    "    s3_path = f'staging/{file_name}'\n",
    "\n",
    "    statinfo = os.stat(full_name)\n",
    "\n",
    "    up_progress = progressbar.progressbar.ProgressBar(maxval=statinfo.st_size)\n",
    "\n",
    "    up_progress.start()\n",
    "\n",
    "    def upload_progress(chunk):\n",
    "        clear_output(wait = True) # Only for IPython (Notebook)\n",
    "        up_progress.update(up_progress.currval + chunk)\n",
    "\n",
    "    response = s3_client.upload_file(full_name, bucket_name, s3_path, Callback=upload_progress)\n",
    "\n",
    "    up_progress.finish()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100% |########################################################################|100% |########################################################################|\n"
    }
   ],
   "source": [
    "# Sync data/loaded folder to s3\n",
    "# https://dev.to/razcodes/how-to-copy-files-to-s3-using-boto3-41fp\n",
    "\n",
    "folder_name = '../data/loading/'\n",
    "directory = os.fsencode(folder_name)\n",
    "\n",
    "# https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    file_name = os.fsdecode(file)\n",
    "    if file_name.endswith(\".json\") or file_name.endswith(\".csv\"): \n",
    "        print(\"Uploading\", file_name)\n",
    "        #full_name = os.path.join(folder_name, file_name)\n",
    "        #response = s3_client.upload_file(full_name, bucket_name, f'staging/{file_name}')\n",
    "        response = upload_file(s3_client, folder_name, file_name, bucket_name)\n",
    "        if response is not None:\n",
    "            print(\"HTTPStatusCode:\", response['ResponseMetadata']['HTTPStatusCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data/loaded folder \n",
    "shutil.rmtree(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit079b8e0c18c84c07a446e24cf94e2db0",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}